#' @title calculate_AUC_base
#' @description the base function of calculate_AUCs: build the model and calculate the AUCs
#'
#' @param n01_all size of all data labeled as class 0/1. For example, n01_all=c(800,800) represents that the size of all data labeled as class 0 is 800 and the size of all data labeled as class 1 is also 800.
#' @param n01_p size of pilot data labeled as class 0/1. For example, n01_p=c(15,15) represents that the size of pilot data labeled as class 0 is 15 and the size of pilot data labeled as class 1 is also 15.
#' @param n_train_sets size sets of training data labeled as class 0/1. For example, n_train_sets=c(c(30,30),c(90,90),c(150,150)) represents that we try 3 different sets of training data and the training size of the first set is c(30,30).
#' @param n01_test number of test data labeled as class 0/1. size of all data labeled as class 0/1. For example, n01_test=c(300,300) represents that the size of test data labeled as class 0 is 300 and the size of test data labeled as class 1 is also 300.
#' @param num_of_seeds number of seeds you want to use to run simulations.
#' @param method
#' Choose the method you want to use: "pca2_mvnorm" and "gaussian_copula".
#' The default value is "pca2_mvnorm".
#' @param model base classification model.
#' \itemize{
#' \item logistic: Logistic regression. \link{glm} function with family = 'binomial'
#' \item penlog: Penalized logistic regression with LASSO penalty. \code{\link[glmnet]{glmnet}} in \code{glmnet} package
#' \item svm: Support Vector Machines. \code{\link[e1071]{svm}} in \code{e1071} package
#' \item randomforest: Random Forest. \code{\link[randomForest]{randomForest}} in \code{randomForest} package
#' \item lda: Linear Discriminant Analysis. \code{\link[MASS]{lda}} in \code{MASS} package
#' \item slda: Sparse Linear Discriminant Analysis with LASSO penalty.
#' \item nb: Naive Bayes. \code{\link[e1071]{naiveBayes}} in \code{e1071} package
#' \item nnb: Nonparametric Naive Bayes. \code{\link[naivebayes]{naive_bayes}} in \code{naivebayes} package
#' \item ada: Ada-Boost. \code{\link[ada]{ada}} in \code{ada} package
#' \item xgboost: XGBboost. \code{\link[xgboost]{xgboost}} in \code{xgboost} package
#' \item tree: Classificatin Tree. \code{\link[tree]{tree}} in \code{tree} package
#' }
#' @param data_generation a parameter list that you can tell the function about the distribution and parameters you want to use to generate the data.
#' \itemize{
#' \item "gaussian" represent multivariate gaussian distribution. see \code{\link[MASS]{mvrnorm}} in \code{MASS} package. For example, data_generation=list(dist="gaussian",sigma=list(class_0=diag(5),class_1=diag(5)),mu=c(rep(0,5),rep(2,5)))
#' \item "t-distribution" represent multivariate t distribution. see \code{\link[mvtnorm]{rmvt}} in \code{mvtnorm} package. For example, data_generation=list(dist="t-distribution",sigma=list(class_0=diag(5),class_1=diag(5)),df=c(10,10),delta=c(rep(0,5),rep(2,5))).
#' }
#'
#' @return Return the AUCs you want to calculate
#' @export
#'
#' @examples AUC = calculate_AUC_base(n01_p=c(15,15), n01_test=c(300,300), n_train_sets = c(c(15,15),c(30,30),c(60,60),c(120,120),c(150,150)), seed=1, model=c("svm","randomforest"))
calculate_AUC_base <- function(n01_all= c(800,800), n01_p=c(15,15), n_train_sets = c(c(15,15),c(30,30),c(60,60),c(120,120),c(150,150)), n01_test=c(300,300), seed=1, method="pca2_mvnorm", model=c("svm","randomforest"),data_generation=list(dist="t-distribution",sigma=list(class_0=diag(5),class_1=diag(5)),df=c(10,10),delta=c(rep(0,5),rep(2,5))))
{
  library(PRROC)
  n0_p <- n01_p[1]
  n1_p <- n01_p[2]
  n_p=n0_p+n1_p

  n0_test <- n01_test[1]
  n1_test <- n01_test[2]
  n_test = n0_test + n1_test

  data = generate_data(seed=seed, n01_all=n01_all, data_generation=data_generation)
  pilot_rest_data = split_data(data$x_data, data$y_data, n_train=n_p, seed=seed)

  test_y=c(rep(0,n0_test),rep(1,n1_test))

  Loop=100

  num_of_model = length(model)

  number_of_train_sets = length(n_train_sets)/2

  dim(n_train_sets) = c(2,number_of_train_sets)

  auc = array(0,dim=c(length(n_train_sets),Loop,num_of_model))
  count = 1
  for (test_from_true in c(0,1)){
    for (i in 1:number_of_train_sets){
      n0_train <- n_train_sets[1,i]
      n1_train <- n_train_sets[2,i]
      n_train = n0_train + n1_train

      p <- array(0,dim=c(Loop,n_test,num_of_model))
      # matrix(0,Loop,n_test)
      for (L in 1:Loop)
      {
        if(test_from_true==0){
          if(method == "pca2_mvnorm"){result = pilot_tfe_mvnorm_pca2(pilot_rest_data$x_train,pilot_rest_data$y_train,n0_train,n1_train,n0_test,n1_test,method=model)}
          if(method == "gaussian_copula"){result = pilot_tfe_gaussian_copula(pilot_rest_data$x_train,pilot_rest_data$y_train,n0_train,n1_train,n0_test,n1_test,method=model)}

        }else if(test_from_true==1){

          train_test_data = split_data(data$x_data, data$y_data, n_train=n_train, n_test=n_test)

          if(method == "pca2_mvnorm"){result = pilot_train_pca2(train_test_data$x_train,train_test_data$y_train,train_test_data$x_test, method=model)}
          if(method == "gaussian_copula"){result = pilot_train_pca2(train_test_data$x_train,train_test_data$y_train,train_test_data$x_test, method=model)}
        }

        for(j in 1:num_of_model){
          p[L,,j] = result[j,]
        }

        cat(count,L,"\n")

      }


      for (k in 1: Loop)
      {
        index0_test=which(test_y==0)
        index1_test=which(test_y==1)

        for(j in 1:num_of_model){
          score = p[k,,j]
          roc <- roc.curve(scores.class0 =score[index1_test],scores.class1=score[index0_test],curve =FALSE)

          auc[count,k,j]=roc$auc
        }

      }
      count = count + 1
    }
  }

  return(auc)
}
