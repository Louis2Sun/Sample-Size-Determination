% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/pilot_train.R
\name{pilot_train}
\alias{pilot_train}
\title{Training the data directly}
\usage{
pilot_train(
  x_train,
  y_train,
  x_test,
  method = c("svm", "randomforest"),
  func = NULL
)
}
\arguments{
\item{x_train}{input variables of training data}

\item{y_train}{labels of training data}

\item{x_test}{input variables of test data}

\item{method}{base classification method.
\itemize{
\item logistic: Logistic regression. \link{glm} function with family = 'binomial'
\item penlog: Penalized logistic regression with LASSO penalty. \code{\link[glmnet]{glmnet}} in \code{glmnet} package
\item svm: Support Vector Machines. \code{\link[e1071]{svm}} in \code{e1071} package
\item randomforest: Random Forest. \code{\link[randomForest]{randomForest}} in \code{randomForest} package
\item lda: Linear Discriminant Analysis. \code{\link[MASS]{lda}} in \code{MASS} package
\item slda: Sparse Linear Discriminant Analysis with LASSO penalty.
\item nb: Naive Bayes. \code{\link[e1071]{naiveBayes}} in \code{e1071} package
\item nnb: Nonparametric Naive Bayes. \code{\link[naivebayes]{naive_bayes}} in \code{naivebayes} package
\item ada: Ada-Boost. \code{\link[ada]{ada}} in \code{ada} package
\item xgboost: XGBboost. \code{\link[xgboost]{xgboost}} in \code{xgboost} package
\item tree: Classificatin Tree. \code{\link[tree]{tree}} in \code{tree} package
\item self: You can use your self-defined function. You need to pass your self-defined function via the "func" parameter.
}}

\item{func}{If you set "method" to "self", you have to pass your self-defined model function. This function should be able to take "x_train" and "y_train" as the first two inputs to train the model and then take "x_test" as the third input and return the predicted scores of x_test data. For example, \cr\cr
\code{library(e1071)\cr\cr
predict_model <- function(x_train, y_train, x_test){ \cr
data_trainxy<-data.frame(x_train,y_train=as.factor(y_train)) \cr
fit_svm<-svm(y_train~.,data=data_trainxy,probability=TRUE) \cr
pred_svm <- predict(fit_svm, x_test, probability=TRUE,decision.values = TRUE) \cr
p_svm=as.data.frame(attr(pred_svm, "probabilities"))$"1" \cr
return(p_svm) \cr
}\cr \cr
result = pilot_train(x_train,y_train,x_test,method=c("self","randomforest"),func=predict_model)}}
}
\value{
the scores predicted by models
}
\description{
use different methods to train the data.
}
\examples{

library(mvtnorm)
library(MASS)

df = 10
rho = 0.5
d = 5
delta = rep(2, d)
H <- abs(outer(1:d, 1:d, "-"))
covxx = rho^H

n1_all <- n0_all <- 800
n1_train <- n0_train <- n_train <- 60
n0_test <- n1_test <- 300

x0_all = rmvt(n = n0_all, sigma = covxx, delta = rep(0, d), df = df)
x1_all = rmvt(n = n1_all, sigma = covxx, delta = delta, df = df)

x_data = rbind(x0_all, x1_all)
y_data = c(rep(0, n0_all), rep(1, n1_all))

id0 <- which(y_data == 0)
id1 <- which(y_data == 1)

id0_train <- sample(id0, n0_train)
id1_train <- sample(id1, n1_train)
id_train <- c(id0_train, id1_train)
x_train <- as.matrix(x_data[id_train, ])
y_train <- as.matrix(y_data[id_train])

id0_remain = setdiff(id0, id0_train)
id1_remain = setdiff(id1, id1_train)

id0_test <- sample(id0_remain, n0_test)
id1_test <- sample(id1_remain, n1_test)
id_test <- c(id0_test, id1_test)
x_test <- as.matrix(x_data[id_test, ])

result = pilot_train(x_train, y_train, x_test, method = c("svm", "randomforest"))
}
